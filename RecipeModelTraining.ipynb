{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\arron\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\arron\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\arron\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\arron\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\arron\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\arron\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\arron\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\arron\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\arron\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: torch in c:\\users\\arron\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\arron\\anaconda3\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\arron\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\arron\\anaconda3\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\arron\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: ipython in c:\\users\\arron\\anaconda3\\lib\\site-packages (8.27.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\arron\\anaconda3\\lib\\site-packages (from ipython) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from ipython) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\arron\\anaconda3\\lib\\site-packages (from ipython) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from ipython) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from ipython) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\arron\\anaconda3\\lib\\site-packages (from ipython) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from ipython) (5.14.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\arron\\anaconda3\\lib\\site-packages (from ipython) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\arron\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\arron\\anaconda3\\lib\\site-packages (from stack-data->ipython) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\arron\\anaconda3\\lib\\site-packages (from stack-data->ipython) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\arron\\anaconda3\\lib\\site-packages (from stack-data->ipython) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\arron\\anaconda3\\lib\\site-packages (from asttokens->stack-data->ipython) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\arron\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: wandb in c:\\users\\arron\\anaconda3\\lib\\site-packages (0.18.7)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\arron\\anaconda3\\lib\\site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\arron\\anaconda3\\lib\\site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from wandb) (2.19.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\arron\\anaconda3\\lib\\site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\arron\\anaconda3\\lib\\site-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arron\\anaconda3\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: torchviz in c:\\users\\arron\\anaconda3\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: torch in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torchviz) (2.5.1)\n",
      "Requirement already satisfied: graphviz in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torchviz) (0.20.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torch->torchviz) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torch->torchviz) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torch->torchviz) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torch->torchviz) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torch->torchviz) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torch->torchviz) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from torch->torchviz) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arron\\anaconda3\\lib\\site-packages (from jinja2->torch->torchviz) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install nltk\n",
    "!pip install scikit-learn\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install tqdm\n",
    "!pip install ipython\n",
    "!pip install matplotlib\n",
    "!pip install wandb\n",
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\arron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cpu\n"
     ]
    }
   ],
   "source": [
    "#dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#normalising characters\n",
    "import unicodedata as ucd\n",
    "\n",
    "#preprocessing data\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "#organising train/test datasets\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit, KFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#ml framework imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, RMSprop\n",
    "\n",
    "#dataset setup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# notebook display\n",
    "from tqdm import tqdm\n",
    "\n",
    "#for clearer printing\n",
    "from IPython.display import Markdown, display\n",
    "from enum import Enum\n",
    "\n",
    "#evaluation metrics\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, confusion_matrix, recall_score\n",
    "\n",
    "#visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from torchviz import make_dot\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stylised Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colours(Enum):\n",
    "    RED = 'red'\n",
    "    BLUE = 'blue'\n",
    "    YELLOW = 'yellow'\n",
    "    GREEN = 'green'\n",
    "\n",
    "class MdTypes(Enum):\n",
    "    HEADING = lambda x: f\"# {x}\"\n",
    "    S_HEADING = lambda x: f\"## {x}\"\n",
    "    SS_HEADING = lambda x: f\"### {x}\"\n",
    "    SSS_HEADING = lambda x: f\"#### {x}\"\n",
    "    BOLD = lambda x: f\"**{x}**\"\n",
    "    ITALIC = lambda x: f\"*{x}*\"\n",
    "    NO_MARK = lambda x: x\n",
    "    \n",
    "def printmd(string, extra = None, md:MdTypes = MdTypes.NO_MARK, colour:Colours = Colours.RED):\n",
    "    string = md(\"<span style='color:{}'>{}</span>\".format(colour.value, string))\n",
    "    if extra:\n",
    "        if not isinstance(extra, list):\n",
    "            string = string + str(extra)\n",
    "        else:\n",
    "            for each in extra:\n",
    "                string = string + \", \" + str(each)          \n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>instruction</th>\n",
       "      <th>image_src</th>\n",
       "      <th>total_time2</th>\n",
       "      <th>difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Veal sweetbreads with ras el hanout, cauliflow...</td>\n",
       "      <td>Marcus Eaves' beautiful sweetbread dish master...</td>\n",
       "      <td>Starter</td>\n",
       "      <td>100g of veal sweetbreads, 70g of T45 flour, 5g...</td>\n",
       "      <td>Start by making the cauliflower crisp to garni...</td>\n",
       "      <td>https://media-cdn2.greatbritishchefs.com/media...</td>\n",
       "      <td>90</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cured sea trout with garden pea, nasturtium an...</td>\n",
       "      <td>Phil Fanning celebrates spring's finest fish w...</td>\n",
       "      <td>Main</td>\n",
       "      <td>250g of 00 flour, 150g of egg yolk, 15g of mil...</td>\n",
       "      <td>To make the pasta, bring all of the ingredient...</td>\n",
       "      <td>https://media-cdn2.greatbritishchefs.com/media...</td>\n",
       "      <td>150</td>\n",
       "      <td>challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grilled fillets of sea bass with herb risotto,...</td>\n",
       "      <td>This grilled sea bass recipe includes a bed of...</td>\n",
       "      <td>Main</td>\n",
       "      <td>4 sea bass fillets, each weighing 140g, ground...</td>\n",
       "      <td>For the herb risotto, melt the butter in a wid...</td>\n",
       "      <td>https://media-cdn2.greatbritishchefs.com/media...</td>\n",
       "      <td>45</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Orange and pink risotto</td>\n",
       "      <td>Josh Eggleton delivers a gorgeous risotto reci...</td>\n",
       "      <td>Main</td>\n",
       "      <td>150g of pearl barley, 4 carrots, 4 red beetroo...</td>\n",
       "      <td>In a juicer, juice 2 of the carrots and set as...</td>\n",
       "      <td>https://media-cdn2.greatbritishchefs.com/media...</td>\n",
       "      <td>90</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Calçots with romesco sauce</td>\n",
       "      <td>A quintessential Catalan pairing that's stood ...</td>\n",
       "      <td>Starter</td>\n",
       "      <td>12 calçots, olive oil, for drizzling, sea sal...</td>\n",
       "      <td>Preheat an oven to 170 C/150 C fan/gas mark 3....</td>\n",
       "      <td>https://media-cdn2.greatbritishchefs.com/media...</td>\n",
       "      <td>30</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Veal sweetbreads with ras el hanout, cauliflow...   \n",
       "1  Cured sea trout with garden pea, nasturtium an...   \n",
       "2  Grilled fillets of sea bass with herb risotto,...   \n",
       "3                            Orange and pink risotto   \n",
       "4                        Calçots with romesco sauce   \n",
       "\n",
       "                                         description category  \\\n",
       "0  Marcus Eaves' beautiful sweetbread dish master...  Starter   \n",
       "1  Phil Fanning celebrates spring's finest fish w...     Main   \n",
       "2  This grilled sea bass recipe includes a bed of...     Main   \n",
       "3  Josh Eggleton delivers a gorgeous risotto reci...     Main   \n",
       "4  A quintessential Catalan pairing that's stood ...  Starter   \n",
       "\n",
       "                                         ingredients  \\\n",
       "0  100g of veal sweetbreads, 70g of T45 flour, 5g...   \n",
       "1  250g of 00 flour, 150g of egg yolk, 15g of mil...   \n",
       "2  4 sea bass fillets, each weighing 140g, ground...   \n",
       "3  150g of pearl barley, 4 carrots, 4 red beetroo...   \n",
       "4  12 calçots, olive oil, for drizzling, sea sal...   \n",
       "\n",
       "                                         instruction  \\\n",
       "0  Start by making the cauliflower crisp to garni...   \n",
       "1  To make the pasta, bring all of the ingredient...   \n",
       "2  For the herb risotto, melt the butter in a wid...   \n",
       "3  In a juicer, juice 2 of the carrots and set as...   \n",
       "4  Preheat an oven to 170 C/150 C fan/gas mark 3....   \n",
       "\n",
       "                                           image_src  total_time2   difficulty  \n",
       "0  https://media-cdn2.greatbritishchefs.com/media...           90       medium  \n",
       "1  https://media-cdn2.greatbritishchefs.com/media...          150  challenging  \n",
       "2  https://media-cdn2.greatbritishchefs.com/media...           45       medium  \n",
       "3  https://media-cdn2.greatbritishchefs.com/media...           90       medium  \n",
       "4  https://media-cdn2.greatbritishchefs.com/media...           30         easy  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"formatted_data_REMOVED_CHARS.csv\")\n",
    "try:\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "    df = df.drop('total_time', axis=1)\n",
    "except:\n",
    "    print(\"attempted to delete non existent collumn\")\n",
    "    \n",
    "cols = df.columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"formatted_data_REMOVED_CHARS_total_time.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods to gather information about character spread and remove unnecessary characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_chars(row):\n",
    "    for each in row:\n",
    "        if not isinstance(each, (int,float)): \n",
    "            if isinstance(each, list):\n",
    "                for item in each:\n",
    "                    #iterate through each character\n",
    "                    for char in item:                      \n",
    "                        #print(\"this is a character\", char)\n",
    "                        if char not in unique_chars:\n",
    "                            unique_chars.append(char)                            \n",
    "                            #print(unique_chars)\n",
    "            else:\n",
    "                #iterate through each character\n",
    "                for char in each:\n",
    "                    if char not in unique_chars:\n",
    "                        unique_chars.append(char)\n",
    "                        #print(unique_chars)\n",
    "    return\n",
    "\n",
    "def get_unique_chars_merged(row):\n",
    "    for item in row[\"merged_text\"]:\n",
    "        for char in item:                 \n",
    "            if char not in unique_chars:\n",
    "                unique_chars.append(char)                            \n",
    "    return\n",
    "\n",
    "#MANUALLY SELECTED CHARACTERS TO REPLACE\n",
    "rep_space = {\" \": ['r\\t', r'\\n', '\\r', '_', '\\xad', '®', '°', '\\u200b', '\\u2028', '\\u2060']}\n",
    "rep_x = {\"x\": ['×']}\n",
    "rep_i = {\"i\": ['ı']}\n",
    "rep_apos = {\"'\": ['ʹ', 'ʼ', '̧̨̛̣̀́̂̃̄̈̉̊̌', '‘', '’', '“', '”', '′', 'ʼ']}\n",
    "rep_empty = {\"\":['̀', '́', '̂', '̃', '̄', '̈', '̉', '̊', '̌', '̛', '̣', '̧', '̨']}\n",
    "rep_fslash = {\"/\": ['⁄']}\n",
    "#REPLACEMENT_DICT IS USED TO REPLACE ALL IDENTIFIED CHARS WITH RELEVANT ALTERNATIVES\n",
    "replacement_dict = {**rep_space, **rep_x, **rep_i, **rep_apos, **rep_fslash}\n",
    "\n",
    "#NORMALIZES UNICODE CHARACTERS\n",
    "def decompose_text(row): \n",
    "    for each in cols:  \n",
    "        if not isinstance(row[each], int): \n",
    "            if isinstance(row[each], list): \n",
    "                #print(\"shouldmt be here\")\n",
    "                for i in range(len(row[each])):                  \n",
    "                    row[each][i] = ucd.normalize('NFKD', str(row[each][i]))\n",
    "                    for reps in replacement_list:\n",
    "                        for ops in replacement_list[reps]:\n",
    "                            if ops in row[each][i]:\n",
    "                                row[each][i] = row[each][i].replace(ops, reps)\n",
    "            else:        \n",
    "                row[each] = ucd.normalize('NFKD', str(row[each]))\n",
    "                for reps in replacement_dict:\n",
    "                        for ops in replacement_dict[reps]:\n",
    "                            if ops in row[each]:\n",
    "                                row[each] = str(row[each].replace(ops, reps))\n",
    "    return row\n",
    "\n",
    "def decompose_merged(field): \n",
    "    field = ucd.normalize('NFKD', str(field))\n",
    "    for reps in replacement_dict:\n",
    "            for ops in replacement_dict[reps]:\n",
    "                if ops in field:\n",
    "                    field = str(field.replace(ops, reps))\n",
    "    return field\n",
    "\n",
    "words = set(nltk.corpus.words.words())\n",
    "def nltk_rem_non_eng(row):\n",
    "    sent = row[\"merged_text\"]\n",
    "    row[\"merged_text\"] = \" \".join(w for w in nltk.wordpunct_tokenize(sent) if w.lower() in words or not w.isalpha())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new dataframe with columns for model usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_text_and_token_list_df(df):\n",
    "    merged_text = (\n",
    "        \"Title: \" + df['title'].astype(str) + \"\\n\" +\n",
    "        \"Description: \" + df['description'].astype(str) + \"\\n\" +\n",
    "        \"Category: \" + df['category'].astype(str) + \"\\n\" +\n",
    "        \"Total Time: \" + df['total_time2'].astype(str) + \"\\n\" +\n",
    "        \"Ingredients: \" + df['ingredients'].astype(str) +\n",
    "        \"Instruction: \" + df['instruction'].astype(str)\n",
    "    )\n",
    "    difficulty_mapping = {\n",
    "    'easy': [1, 0, 0],\n",
    "    'medium': [0, 1, 0],\n",
    "    'challenging': [0, 0, 1]\n",
    "    }\n",
    "    new_df = pd.DataFrame({'merged_text': merged_text, 'token_list': None, 'numerical_token': None, 'difficulty': df['difficulty'], 'label':df['difficulty'].map(difficulty_mapping)})\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing chars and updating the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nreview_df1 = merged_text_and_token_list_df(df)\\nreview_df2 = merged_text_and_token_list_df(df)\\n\\nunique_chars = []\\nreview_df1.apply(get_unique_chars_merged,axis=1)\\nprint(\"Initial Characters:\\n\", sorted(unique_chars))\\n\\nreview_df1.apply(decompose_merged,axis=1)\\nunique_chars = []\\nreview_df1.apply(get_unique_chars_merged, axis=1)\\nprint(\"Manual char removal:\\n\", sorted(unique_chars))\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##LIGATURES HAVE ALREADY BEEN REMOVED PREVIOUSLY##\n",
    "\"\"\"\n",
    "review_df1 = merged_text_and_token_list_df(df)\n",
    "review_df2 = merged_text_and_token_list_df(df)\n",
    "\n",
    "unique_chars = []\n",
    "review_df1.apply(get_unique_chars_merged,axis=1)\n",
    "print(\"Initial Characters:\\n\", sorted(unique_chars))\n",
    "\n",
    "review_df1.apply(decompose_merged,axis=1)\n",
    "unique_chars = []\n",
    "review_df1.apply(get_unique_chars_merged, axis=1)\n",
    "print(\"Manual char removal:\\n\", sorted(unique_chars))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenization and preprocessing over dataframe and single input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBAL DEF OF PREPROCESSING COMPONENTS\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "def tokenize_and_process(tok_df):\n",
    "    \n",
    "    tok_df[\"merged_text\"] = tok_df[\"merged_text\"].apply(decompose_merged)\n",
    "    printmd(\"Merged text Index 0 --manual char removal--\\n\", tok_df[\"merged_text\"].iloc[0])\n",
    "    #tokenisation\n",
    "    if isinstance(tok_df[\"merged_text\"].iloc[0], str):\n",
    "        tok_df[\"token_list\"] = tok_df['merged_text'].apply(lambda x: word_tokenize(x))\n",
    "    printmd(\"\\nMerged text Index 0 --tokenisation--\\n\", tok_df[\"merged_text\"].iloc[0])\n",
    "    printmd(\"\\nToken list Index 0 --tokenisation--\\n\", tok_df[\"token_list\"].iloc[0])\n",
    "    \n",
    "    #Lowercasing\n",
    "    tok_df[\"token_list\"] = tok_df[\"token_list\"].apply(lambda x: [token.lower() for token in x])\n",
    "    printmd(\"\\nToken list Index 0 --lowercasing--\\n\", tok_df[\"token_list\"].iloc[0])\n",
    "    \n",
    "    #Stop word removal\n",
    "    tok_df[\"token_list\"] = tok_df[\"token_list\"] .apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    printmd(\"\\nToken list Index 0 --Stop Word Removal--\\n\", tok_df[\"token_list\"].iloc[0])\n",
    "    \n",
    "    tl_pre_lem = tok_df.iloc[0][\"token_list\"]\n",
    "    \n",
    "    ##lemmatisation\n",
    "    tok_df[\"token_list\"] = tok_df[\"token_list\"] .apply(lemmatize_tokens)\n",
    "    printmd(\"\\nToken list Index 0 --lemmatization--\\n\", tok_df[\"token_list\"].iloc[0])\n",
    "\n",
    "    #check for differences after lemmatisation\n",
    "    tl_l_post_lem = tok_df.iloc[0][\"token_list\"]\n",
    "    count = 0\n",
    "    for i in range(len(tl_pre_lem)):\n",
    "        if tl_pre_lem[i] != tl_l_post_lem[i]:\n",
    "            count +=1\n",
    "    printmd(\"\\n there are\", [str(count) ,\" differences after lemmatisation\"],colour = Colours.BLUE)\n",
    "    return tok_df\n",
    "\n",
    "def t_and_p_new_input(input_v):\n",
    "    iv = decompose_merged(input_v)\n",
    "    iv = word_tokenize(iv)\n",
    "    iv = [token.lower() for token in iv]\n",
    "    iv = [word for word in iv if word not in stop_words]\n",
    "    iv = lemmatize_tokens(iv)\n",
    "    return iv\n",
    "\n",
    "##### UNUSED PREPROCESSING METHODS #####\n",
    "#Remove whitespace?\n",
    "#Remove frequent words (not relevant due to the number of techniques that are mentioned within models).\n",
    "#Spelling correction\n",
    "#Remove punctuation (not useful due to list nature)\n",
    "#POS tagging (research why that may be useful)\n",
    "#Named entity recognition - (useful for ingredients maybe)\n",
    "#Phrase normalisation (research more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='color:red'>Merged text Index 0 --manual char removal--\n",
       "</span>Title: Veal sweetbreads with ras el hanout, cauliflower purée, watercress and coriander\n",
       "Description: Marcus Eaves' beautiful sweetbread dish masterfully balances delicately flavoured sweetbreads and cauliflower with bright and bold coriander, pomegranate and mint.\n",
       "Category: Starter\n",
       "Total Time: 90\n",
       "Ingredients: 100g of veal sweetbreads, 70g of T45 flour, 5g of ras el hanout, 1 garlic clove, 1 sprig of thyme, 1 tbsp of butter, plus beurre noisette butter, 1 tsp pine nuts, parsley, chopped, lemon juice, vegetable oil, salt, pepper, 200g of cauliflower, finely chopped, 150g of milk, 100g of double cream, 3g of salt, 120ml of lemon olive oil, 30g of lemon vinegar, salt to taste, 150g of cauliflower, grated on a microplane, salt, 2 tbsp of golden raisins, soaked, 1 tbsp of pine nuts, 1 tbsp of pomegranate seeds, mint leaves, chopped, coriander, finely chopped, 1 cauliflower, butter, 30ml of chicken stock, baby watercress, washed and picked, coriander, olive oilInstruction: Start by making the cauliflower crisp to garnish the dish. Use a mandoline to cut 2mm thick slices of half of the cauliflower set aside for garnish. Lay the slices out onto a non-stick mat and place in a dehydrator at 60 C until the cauliflower is crisp, for approximately 4-6 hours. For the cauliflower purée, add the milk, double cream and salt to a pan and bring to a simmer. Add the cauliflower and cook until tender and soft to the touch, this will take 5-10 minutes. Allow to cook until about half of the liquid has evaporated, then tip the contents of the pan into a strainer, saving the liquid. Blend the cauliflower with half of the remaining liquid, adding more to achieve the desired consistency. Pass through a fine sieve and set aside until ready to serve. For the lemon vinaigrette, simply whisk together the ingredients until smooth. Ensure to mix well again before using later. For the cauliflower couscous, cook the cauliflower in salted boiling water until tender for 2-3 minutes, strain and refresh in ice water. Drain and mix in the golden raisins and pine nuts. Toss to combine and season to taste with the lemon vinaigrette. Set aside. To cook the sweetbreads, preheat a water bath to 63 ̊C. Season evenly with salt and pepper and vacuum seal. Cook in the water bath for 11 minutes and cool in ice cold water. Then, remove from the pouch and pat dry. Season the cooked sweetbreads and roll in a mix of the flour and ras el hanout. Add a generous film of olive oil to a pan and place over a high heat. Once hot, caramelise the sweetbreads on one side and turn over. Add a tablespoon of the butter, the garlic and the thyme sprigs and use a spoon to continuously baste the sweetbreads in the foaming butter for another 1-2 minutes. Remove the sweetbreads from the pan, rest for 45 seconds then carve. Drain the fat from the pan, wipe dry with kitchen paper and return to the heat. Make a beurre noisette with another knob of butter and add the pine nuts, chopped parsley and a generous squeeze of lemon juice. Remove from the heat and set aside. Cut the remaining cauliflower half for garnish into medium sized florets. Blanch the pieces of cauliflower in salted boiling water for up to 1 minute, then remove and set aside to dry. Place a small pan over a medium heat and add a knob of butter and the chicken stock. Reduce to a glaze, stirring with a wooden spoon to maintain its texture. Remove from heat and place in a small bowl. Roll the blanched cauliflower through this chicken stock and butter emulsion until well-coated. To serve, add the fresh pomegranate, mint and coriander to the cauliflower couscous. Place one heaped tablespoon of the couscous on each plate and place the buttered cauliflower around the edge. Add slices of sweetbread to cover the cauliflower couscous. Dress the sweetbreads with the beurre noisette and decorate the plate with baby watercress, picked coriander leaves and dehydrated cauliflower. Finish with quenelles of the cauliflower purée and a drizzle of olive oil"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:red'>\n",
       "Merged text Index 0 --tokenisation--\n",
       "</span>Title: Veal sweetbreads with ras el hanout, cauliflower purée, watercress and coriander\n",
       "Description: Marcus Eaves' beautiful sweetbread dish masterfully balances delicately flavoured sweetbreads and cauliflower with bright and bold coriander, pomegranate and mint.\n",
       "Category: Starter\n",
       "Total Time: 90\n",
       "Ingredients: 100g of veal sweetbreads, 70g of T45 flour, 5g of ras el hanout, 1 garlic clove, 1 sprig of thyme, 1 tbsp of butter, plus beurre noisette butter, 1 tsp pine nuts, parsley, chopped, lemon juice, vegetable oil, salt, pepper, 200g of cauliflower, finely chopped, 150g of milk, 100g of double cream, 3g of salt, 120ml of lemon olive oil, 30g of lemon vinegar, salt to taste, 150g of cauliflower, grated on a microplane, salt, 2 tbsp of golden raisins, soaked, 1 tbsp of pine nuts, 1 tbsp of pomegranate seeds, mint leaves, chopped, coriander, finely chopped, 1 cauliflower, butter, 30ml of chicken stock, baby watercress, washed and picked, coriander, olive oilInstruction: Start by making the cauliflower crisp to garnish the dish. Use a mandoline to cut 2mm thick slices of half of the cauliflower set aside for garnish. Lay the slices out onto a non-stick mat and place in a dehydrator at 60 C until the cauliflower is crisp, for approximately 4-6 hours. For the cauliflower purée, add the milk, double cream and salt to a pan and bring to a simmer. Add the cauliflower and cook until tender and soft to the touch, this will take 5-10 minutes. Allow to cook until about half of the liquid has evaporated, then tip the contents of the pan into a strainer, saving the liquid. Blend the cauliflower with half of the remaining liquid, adding more to achieve the desired consistency. Pass through a fine sieve and set aside until ready to serve. For the lemon vinaigrette, simply whisk together the ingredients until smooth. Ensure to mix well again before using later. For the cauliflower couscous, cook the cauliflower in salted boiling water until tender for 2-3 minutes, strain and refresh in ice water. Drain and mix in the golden raisins and pine nuts. Toss to combine and season to taste with the lemon vinaigrette. Set aside. To cook the sweetbreads, preheat a water bath to 63 ̊C. Season evenly with salt and pepper and vacuum seal. Cook in the water bath for 11 minutes and cool in ice cold water. Then, remove from the pouch and pat dry. Season the cooked sweetbreads and roll in a mix of the flour and ras el hanout. Add a generous film of olive oil to a pan and place over a high heat. Once hot, caramelise the sweetbreads on one side and turn over. Add a tablespoon of the butter, the garlic and the thyme sprigs and use a spoon to continuously baste the sweetbreads in the foaming butter for another 1-2 minutes. Remove the sweetbreads from the pan, rest for 45 seconds then carve. Drain the fat from the pan, wipe dry with kitchen paper and return to the heat. Make a beurre noisette with another knob of butter and add the pine nuts, chopped parsley and a generous squeeze of lemon juice. Remove from the heat and set aside. Cut the remaining cauliflower half for garnish into medium sized florets. Blanch the pieces of cauliflower in salted boiling water for up to 1 minute, then remove and set aside to dry. Place a small pan over a medium heat and add a knob of butter and the chicken stock. Reduce to a glaze, stirring with a wooden spoon to maintain its texture. Remove from heat and place in a small bowl. Roll the blanched cauliflower through this chicken stock and butter emulsion until well-coated. To serve, add the fresh pomegranate, mint and coriander to the cauliflower couscous. Place one heaped tablespoon of the couscous on each plate and place the buttered cauliflower around the edge. Add slices of sweetbread to cover the cauliflower couscous. Dress the sweetbreads with the beurre noisette and decorate the plate with baby watercress, picked coriander leaves and dehydrated cauliflower. Finish with quenelles of the cauliflower purée and a drizzle of olive oil"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:red'>\n",
       "Token list Index 0 --tokenisation--\n",
       "</span>, Title, :, Veal, sweetbreads, with, ras, el, hanout, ,, cauliflower, purée, ,, watercress, and, coriander, Description, :, Marcus, Eaves, ', beautiful, sweetbread, dish, masterfully, balances, delicately, flavoured, sweetbreads, and, cauliflower, with, bright, and, bold, coriander, ,, pomegranate, and, mint, ., Category, :, Starter, Total, Time, :, 90, Ingredients, :, 100g, of, veal, sweetbreads, ,, 70g, of, T45, flour, ,, 5g, of, ras, el, hanout, ,, 1, garlic, clove, ,, 1, sprig, of, thyme, ,, 1, tbsp, of, butter, ,, plus, beurre, noisette, butter, ,, 1, tsp, pine, nuts, ,, parsley, ,, chopped, ,, lemon, juice, ,, vegetable, oil, ,, salt, ,, pepper, ,, 200g, of, cauliflower, ,, finely, chopped, ,, 150g, of, milk, ,, 100g, of, double, cream, ,, 3g, of, salt, ,, 120ml, of, lemon, olive, oil, ,, 30g, of, lemon, vinegar, ,, salt, to, taste, ,, 150g, of, cauliflower, ,, grated, on, a, microplane, ,, salt, ,, 2, tbsp, of, golden, raisins, ,, soaked, ,, 1, tbsp, of, pine, nuts, ,, 1, tbsp, of, pomegranate, seeds, ,, mint, leaves, ,, chopped, ,, coriander, ,, finely, chopped, ,, 1, cauliflower, ,, butter, ,, 30ml, of, chicken, stock, ,, baby, watercress, ,, washed, and, picked, ,, coriander, ,, olive, oilInstruction, :, Start, by, making, the, cauliflower, crisp, to, garnish, the, dish, ., Use, a, mandoline, to, cut, 2mm, thick, slices, of, half, of, the, cauliflower, set, aside, for, garnish, ., Lay, the, slices, out, onto, a, non-stick, mat, and, place, in, a, dehydrator, at, 60, C, until, the, cauliflower, is, crisp, ,, for, approximately, 4-6, hours, ., For, the, cauliflower, purée, ,, add, the, milk, ,, double, cream, and, salt, to, a, pan, and, bring, to, a, simmer, ., Add, the, cauliflower, and, cook, until, tender, and, soft, to, the, touch, ,, this, will, take, 5-10, minutes, ., Allow, to, cook, until, about, half, of, the, liquid, has, evaporated, ,, then, tip, the, contents, of, the, pan, into, a, strainer, ,, saving, the, liquid, ., Blend, the, cauliflower, with, half, of, the, remaining, liquid, ,, adding, more, to, achieve, the, desired, consistency, ., Pass, through, a, fine, sieve, and, set, aside, until, ready, to, serve, ., For, the, lemon, vinaigrette, ,, simply, whisk, together, the, ingredients, until, smooth, ., Ensure, to, mix, well, again, before, using, later, ., For, the, cauliflower, couscous, ,, cook, the, cauliflower, in, salted, boiling, water, until, tender, for, 2-3, minutes, ,, strain, and, refresh, in, ice, water, ., Drain, and, mix, in, the, golden, raisins, and, pine, nuts, ., Toss, to, combine, and, season, to, taste, with, the, lemon, vinaigrette, ., Set, aside, ., To, cook, the, sweetbreads, ,, preheat, a, water, bath, to, 63, ̊C, ., Season, evenly, with, salt, and, pepper, and, vacuum, seal, ., Cook, in, the, water, bath, for, 11, minutes, and, cool, in, ice, cold, water, ., Then, ,, remove, from, the, pouch, and, pat, dry, ., Season, the, cooked, sweetbreads, and, roll, in, a, mix, of, the, flour, and, ras, el, hanout, ., Add, a, generous, film, of, olive, oil, to, a, pan, and, place, over, a, high, heat, ., Once, hot, ,, caramelise, the, sweetbreads, on, one, side, and, turn, over, ., Add, a, tablespoon, of, the, butter, ,, the, garlic, and, the, thyme, sprigs, and, use, a, spoon, to, continuously, baste, the, sweetbreads, in, the, foaming, butter, for, another, 1-2, minutes, ., Remove, the, sweetbreads, from, the, pan, ,, rest, for, 45, seconds, then, carve, ., Drain, the, fat, from, the, pan, ,, wipe, dry, with, kitchen, paper, and, return, to, the, heat, ., Make, a, beurre, noisette, with, another, knob, of, butter, and, add, the, pine, nuts, ,, chopped, parsley, and, a, generous, squeeze, of, lemon, juice, ., Remove, from, the, heat, and, set, aside, ., Cut, the, remaining, cauliflower, half, for, garnish, into, medium, sized, florets, ., Blanch, the, pieces, of, cauliflower, in, salted, boiling, water, for, up, to, 1, minute, ,, then, remove, and, set, aside, to, dry, ., Place, a, small, pan, over, a, medium, heat, and, add, a, knob, of, butter, and, the, chicken, stock, ., Reduce, to, a, glaze, ,, stirring, with, a, wooden, spoon, to, maintain, its, texture, ., Remove, from, heat, and, place, in, a, small, bowl, ., Roll, the, blanched, cauliflower, through, this, chicken, stock, and, butter, emulsion, until, well-coated, ., To, serve, ,, add, the, fresh, pomegranate, ,, mint, and, coriander, to, the, cauliflower, couscous, ., Place, one, heaped, tablespoon, of, the, couscous, on, each, plate, and, place, the, buttered, cauliflower, around, the, edge, ., Add, slices, of, sweetbread, to, cover, the, cauliflower, couscous, ., Dress, the, sweetbreads, with, the, beurre, noisette, and, decorate, the, plate, with, baby, watercress, ,, picked, coriander, leaves, and, dehydrated, cauliflower, ., Finish, with, quenelles, of, the, cauliflower, purée, and, a, drizzle, of, olive, oil"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:red'>\n",
       "Token list Index 0 --lowercasing--\n",
       "</span>, title, :, veal, sweetbreads, with, ras, el, hanout, ,, cauliflower, purée, ,, watercress, and, coriander, description, :, marcus, eaves, ', beautiful, sweetbread, dish, masterfully, balances, delicately, flavoured, sweetbreads, and, cauliflower, with, bright, and, bold, coriander, ,, pomegranate, and, mint, ., category, :, starter, total, time, :, 90, ingredients, :, 100g, of, veal, sweetbreads, ,, 70g, of, t45, flour, ,, 5g, of, ras, el, hanout, ,, 1, garlic, clove, ,, 1, sprig, of, thyme, ,, 1, tbsp, of, butter, ,, plus, beurre, noisette, butter, ,, 1, tsp, pine, nuts, ,, parsley, ,, chopped, ,, lemon, juice, ,, vegetable, oil, ,, salt, ,, pepper, ,, 200g, of, cauliflower, ,, finely, chopped, ,, 150g, of, milk, ,, 100g, of, double, cream, ,, 3g, of, salt, ,, 120ml, of, lemon, olive, oil, ,, 30g, of, lemon, vinegar, ,, salt, to, taste, ,, 150g, of, cauliflower, ,, grated, on, a, microplane, ,, salt, ,, 2, tbsp, of, golden, raisins, ,, soaked, ,, 1, tbsp, of, pine, nuts, ,, 1, tbsp, of, pomegranate, seeds, ,, mint, leaves, ,, chopped, ,, coriander, ,, finely, chopped, ,, 1, cauliflower, ,, butter, ,, 30ml, of, chicken, stock, ,, baby, watercress, ,, washed, and, picked, ,, coriander, ,, olive, oilinstruction, :, start, by, making, the, cauliflower, crisp, to, garnish, the, dish, ., use, a, mandoline, to, cut, 2mm, thick, slices, of, half, of, the, cauliflower, set, aside, for, garnish, ., lay, the, slices, out, onto, a, non-stick, mat, and, place, in, a, dehydrator, at, 60, c, until, the, cauliflower, is, crisp, ,, for, approximately, 4-6, hours, ., for, the, cauliflower, purée, ,, add, the, milk, ,, double, cream, and, salt, to, a, pan, and, bring, to, a, simmer, ., add, the, cauliflower, and, cook, until, tender, and, soft, to, the, touch, ,, this, will, take, 5-10, minutes, ., allow, to, cook, until, about, half, of, the, liquid, has, evaporated, ,, then, tip, the, contents, of, the, pan, into, a, strainer, ,, saving, the, liquid, ., blend, the, cauliflower, with, half, of, the, remaining, liquid, ,, adding, more, to, achieve, the, desired, consistency, ., pass, through, a, fine, sieve, and, set, aside, until, ready, to, serve, ., for, the, lemon, vinaigrette, ,, simply, whisk, together, the, ingredients, until, smooth, ., ensure, to, mix, well, again, before, using, later, ., for, the, cauliflower, couscous, ,, cook, the, cauliflower, in, salted, boiling, water, until, tender, for, 2-3, minutes, ,, strain, and, refresh, in, ice, water, ., drain, and, mix, in, the, golden, raisins, and, pine, nuts, ., toss, to, combine, and, season, to, taste, with, the, lemon, vinaigrette, ., set, aside, ., to, cook, the, sweetbreads, ,, preheat, a, water, bath, to, 63, ̊c, ., season, evenly, with, salt, and, pepper, and, vacuum, seal, ., cook, in, the, water, bath, for, 11, minutes, and, cool, in, ice, cold, water, ., then, ,, remove, from, the, pouch, and, pat, dry, ., season, the, cooked, sweetbreads, and, roll, in, a, mix, of, the, flour, and, ras, el, hanout, ., add, a, generous, film, of, olive, oil, to, a, pan, and, place, over, a, high, heat, ., once, hot, ,, caramelise, the, sweetbreads, on, one, side, and, turn, over, ., add, a, tablespoon, of, the, butter, ,, the, garlic, and, the, thyme, sprigs, and, use, a, spoon, to, continuously, baste, the, sweetbreads, in, the, foaming, butter, for, another, 1-2, minutes, ., remove, the, sweetbreads, from, the, pan, ,, rest, for, 45, seconds, then, carve, ., drain, the, fat, from, the, pan, ,, wipe, dry, with, kitchen, paper, and, return, to, the, heat, ., make, a, beurre, noisette, with, another, knob, of, butter, and, add, the, pine, nuts, ,, chopped, parsley, and, a, generous, squeeze, of, lemon, juice, ., remove, from, the, heat, and, set, aside, ., cut, the, remaining, cauliflower, half, for, garnish, into, medium, sized, florets, ., blanch, the, pieces, of, cauliflower, in, salted, boiling, water, for, up, to, 1, minute, ,, then, remove, and, set, aside, to, dry, ., place, a, small, pan, over, a, medium, heat, and, add, a, knob, of, butter, and, the, chicken, stock, ., reduce, to, a, glaze, ,, stirring, with, a, wooden, spoon, to, maintain, its, texture, ., remove, from, heat, and, place, in, a, small, bowl, ., roll, the, blanched, cauliflower, through, this, chicken, stock, and, butter, emulsion, until, well-coated, ., to, serve, ,, add, the, fresh, pomegranate, ,, mint, and, coriander, to, the, cauliflower, couscous, ., place, one, heaped, tablespoon, of, the, couscous, on, each, plate, and, place, the, buttered, cauliflower, around, the, edge, ., add, slices, of, sweetbread, to, cover, the, cauliflower, couscous, ., dress, the, sweetbreads, with, the, beurre, noisette, and, decorate, the, plate, with, baby, watercress, ,, picked, coriander, leaves, and, dehydrated, cauliflower, ., finish, with, quenelles, of, the, cauliflower, purée, and, a, drizzle, of, olive, oil"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:red'>\n",
       "Token list Index 0 --Stop Word Removal--\n",
       "</span>, title, :, veal, sweetbreads, ras, el, hanout, ,, cauliflower, purée, ,, watercress, coriander, description, :, marcus, eaves, ', beautiful, sweetbread, dish, masterfully, balances, delicately, flavoured, sweetbreads, cauliflower, bright, bold, coriander, ,, pomegranate, mint, ., category, :, starter, total, time, :, 90, ingredients, :, 100g, veal, sweetbreads, ,, 70g, t45, flour, ,, 5g, ras, el, hanout, ,, 1, garlic, clove, ,, 1, sprig, thyme, ,, 1, tbsp, butter, ,, plus, beurre, noisette, butter, ,, 1, tsp, pine, nuts, ,, parsley, ,, chopped, ,, lemon, juice, ,, vegetable, oil, ,, salt, ,, pepper, ,, 200g, cauliflower, ,, finely, chopped, ,, 150g, milk, ,, 100g, double, cream, ,, 3g, salt, ,, 120ml, lemon, olive, oil, ,, 30g, lemon, vinegar, ,, salt, taste, ,, 150g, cauliflower, ,, grated, microplane, ,, salt, ,, 2, tbsp, golden, raisins, ,, soaked, ,, 1, tbsp, pine, nuts, ,, 1, tbsp, pomegranate, seeds, ,, mint, leaves, ,, chopped, ,, coriander, ,, finely, chopped, ,, 1, cauliflower, ,, butter, ,, 30ml, chicken, stock, ,, baby, watercress, ,, washed, picked, ,, coriander, ,, olive, oilinstruction, :, start, making, cauliflower, crisp, garnish, dish, ., use, mandoline, cut, 2mm, thick, slices, half, cauliflower, set, aside, garnish, ., lay, slices, onto, non-stick, mat, place, dehydrator, 60, c, cauliflower, crisp, ,, approximately, 4-6, hours, ., cauliflower, purée, ,, add, milk, ,, double, cream, salt, pan, bring, simmer, ., add, cauliflower, cook, tender, soft, touch, ,, take, 5-10, minutes, ., allow, cook, half, liquid, evaporated, ,, tip, contents, pan, strainer, ,, saving, liquid, ., blend, cauliflower, half, remaining, liquid, ,, adding, achieve, desired, consistency, ., pass, fine, sieve, set, aside, ready, serve, ., lemon, vinaigrette, ,, simply, whisk, together, ingredients, smooth, ., ensure, mix, well, using, later, ., cauliflower, couscous, ,, cook, cauliflower, salted, boiling, water, tender, 2-3, minutes, ,, strain, refresh, ice, water, ., drain, mix, golden, raisins, pine, nuts, ., toss, combine, season, taste, lemon, vinaigrette, ., set, aside, ., cook, sweetbreads, ,, preheat, water, bath, 63, ̊c, ., season, evenly, salt, pepper, vacuum, seal, ., cook, water, bath, 11, minutes, cool, ice, cold, water, ., ,, remove, pouch, pat, dry, ., season, cooked, sweetbreads, roll, mix, flour, ras, el, hanout, ., add, generous, film, olive, oil, pan, place, high, heat, ., hot, ,, caramelise, sweetbreads, one, side, turn, ., add, tablespoon, butter, ,, garlic, thyme, sprigs, use, spoon, continuously, baste, sweetbreads, foaming, butter, another, 1-2, minutes, ., remove, sweetbreads, pan, ,, rest, 45, seconds, carve, ., drain, fat, pan, ,, wipe, dry, kitchen, paper, return, heat, ., make, beurre, noisette, another, knob, butter, add, pine, nuts, ,, chopped, parsley, generous, squeeze, lemon, juice, ., remove, heat, set, aside, ., cut, remaining, cauliflower, half, garnish, medium, sized, florets, ., blanch, pieces, cauliflower, salted, boiling, water, 1, minute, ,, remove, set, aside, dry, ., place, small, pan, medium, heat, add, knob, butter, chicken, stock, ., reduce, glaze, ,, stirring, wooden, spoon, maintain, texture, ., remove, heat, place, small, bowl, ., roll, blanched, cauliflower, chicken, stock, butter, emulsion, well-coated, ., serve, ,, add, fresh, pomegranate, ,, mint, coriander, cauliflower, couscous, ., place, one, heaped, tablespoon, couscous, plate, place, buttered, cauliflower, around, edge, ., add, slices, sweetbread, cover, cauliflower, couscous, ., dress, sweetbreads, beurre, noisette, decorate, plate, baby, watercress, ,, picked, coriander, leaves, dehydrated, cauliflower, ., finish, quenelles, cauliflower, purée, drizzle, olive, oil"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:red'>\n",
       "Token list Index 0 --lemmatization--\n",
       "</span>, title, :, veal, sweetbread, ra, el, hanout, ,, cauliflower, purée, ,, watercress, coriander, description, :, marcus, eaves, ', beautiful, sweetbread, dish, masterfully, balance, delicately, flavoured, sweetbread, cauliflower, bright, bold, coriander, ,, pomegranate, mint, ., category, :, starter, total, time, :, 90, ingredient, :, 100g, veal, sweetbread, ,, 70g, t45, flour, ,, 5g, ra, el, hanout, ,, 1, garlic, clove, ,, 1, sprig, thyme, ,, 1, tbsp, butter, ,, plus, beurre, noisette, butter, ,, 1, tsp, pine, nut, ,, parsley, ,, chopped, ,, lemon, juice, ,, vegetable, oil, ,, salt, ,, pepper, ,, 200g, cauliflower, ,, finely, chopped, ,, 150g, milk, ,, 100g, double, cream, ,, 3g, salt, ,, 120ml, lemon, olive, oil, ,, 30g, lemon, vinegar, ,, salt, taste, ,, 150g, cauliflower, ,, grated, microplane, ,, salt, ,, 2, tbsp, golden, raisin, ,, soaked, ,, 1, tbsp, pine, nut, ,, 1, tbsp, pomegranate, seed, ,, mint, leaf, ,, chopped, ,, coriander, ,, finely, chopped, ,, 1, cauliflower, ,, butter, ,, 30ml, chicken, stock, ,, baby, watercress, ,, washed, picked, ,, coriander, ,, olive, oilinstruction, :, start, making, cauliflower, crisp, garnish, dish, ., use, mandoline, cut, 2mm, thick, slice, half, cauliflower, set, aside, garnish, ., lay, slice, onto, non-stick, mat, place, dehydrator, 60, c, cauliflower, crisp, ,, approximately, 4-6, hour, ., cauliflower, purée, ,, add, milk, ,, double, cream, salt, pan, bring, simmer, ., add, cauliflower, cook, tender, soft, touch, ,, take, 5-10, minute, ., allow, cook, half, liquid, evaporated, ,, tip, content, pan, strainer, ,, saving, liquid, ., blend, cauliflower, half, remaining, liquid, ,, adding, achieve, desired, consistency, ., pas, fine, sieve, set, aside, ready, serve, ., lemon, vinaigrette, ,, simply, whisk, together, ingredient, smooth, ., ensure, mix, well, using, later, ., cauliflower, couscous, ,, cook, cauliflower, salted, boiling, water, tender, 2-3, minute, ,, strain, refresh, ice, water, ., drain, mix, golden, raisin, pine, nut, ., toss, combine, season, taste, lemon, vinaigrette, ., set, aside, ., cook, sweetbread, ,, preheat, water, bath, 63, ̊c, ., season, evenly, salt, pepper, vacuum, seal, ., cook, water, bath, 11, minute, cool, ice, cold, water, ., ,, remove, pouch, pat, dry, ., season, cooked, sweetbread, roll, mix, flour, ra, el, hanout, ., add, generous, film, olive, oil, pan, place, high, heat, ., hot, ,, caramelise, sweetbread, one, side, turn, ., add, tablespoon, butter, ,, garlic, thyme, sprig, use, spoon, continuously, baste, sweetbread, foaming, butter, another, 1-2, minute, ., remove, sweetbread, pan, ,, rest, 45, second, carve, ., drain, fat, pan, ,, wipe, dry, kitchen, paper, return, heat, ., make, beurre, noisette, another, knob, butter, add, pine, nut, ,, chopped, parsley, generous, squeeze, lemon, juice, ., remove, heat, set, aside, ., cut, remaining, cauliflower, half, garnish, medium, sized, floret, ., blanch, piece, cauliflower, salted, boiling, water, 1, minute, ,, remove, set, aside, dry, ., place, small, pan, medium, heat, add, knob, butter, chicken, stock, ., reduce, glaze, ,, stirring, wooden, spoon, maintain, texture, ., remove, heat, place, small, bowl, ., roll, blanched, cauliflower, chicken, stock, butter, emulsion, well-coated, ., serve, ,, add, fresh, pomegranate, ,, mint, coriander, cauliflower, couscous, ., place, one, heaped, tablespoon, couscous, plate, place, buttered, cauliflower, around, edge, ., add, slice, sweetbread, cover, cauliflower, couscous, ., dress, sweetbread, beurre, noisette, decorate, plate, baby, watercress, ,, picked, coriander, leaf, dehydrated, cauliflower, ., finish, quenelles, cauliflower, purée, drizzle, olive, oil"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:blue'>\n",
       " there are</span>, 38,  differences after lemmatisation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merged_text</th>\n",
       "      <th>token_list</th>\n",
       "      <th>numerical_token</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Title: Veal sweetbreads with ras el hanout, ca...</td>\n",
       "      <td>[title, :, veal, sweetbread, ra, el, hanout, ,...</td>\n",
       "      <td>None</td>\n",
       "      <td>medium</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title: Cured sea trout with garden pea, nastur...</td>\n",
       "      <td>[title, :, cured, sea, trout, garden, pea, ,, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>challenging</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Title: Grilled fillets of sea bass with herb r...</td>\n",
       "      <td>[title, :, grilled, fillet, sea, bass, herb, r...</td>\n",
       "      <td>None</td>\n",
       "      <td>medium</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Title: Orange and pink risotto\\nDescription: J...</td>\n",
       "      <td>[title, :, orange, pink, risotto, description,...</td>\n",
       "      <td>None</td>\n",
       "      <td>medium</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Title: Calçots with romesco sauce\\nDescriptio...</td>\n",
       "      <td>[title, :, calçots, romesco, sauce, descripti...</td>\n",
       "      <td>None</td>\n",
       "      <td>easy</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         merged_text  \\\n",
       "0  Title: Veal sweetbreads with ras el hanout, ca...   \n",
       "1  Title: Cured sea trout with garden pea, nastur...   \n",
       "2  Title: Grilled fillets of sea bass with herb r...   \n",
       "3  Title: Orange and pink risotto\\nDescription: J...   \n",
       "4  Title: Calçots with romesco sauce\\nDescriptio...   \n",
       "\n",
       "                                          token_list numerical_token  \\\n",
       "0  [title, :, veal, sweetbread, ra, el, hanout, ,...            None   \n",
       "1  [title, :, cured, sea, trout, garden, pea, ,, ...            None   \n",
       "2  [title, :, grilled, fillet, sea, bass, herb, r...            None   \n",
       "3  [title, :, orange, pink, risotto, description,...            None   \n",
       "4  [title, :, calçots, romesco, sauce, descripti...            None   \n",
       "\n",
       "    difficulty      label  \n",
       "0       medium  [0, 1, 0]  \n",
       "1  challenging  [0, 0, 1]  \n",
       "2       medium  [0, 1, 0]  \n",
       "3       medium  [0, 1, 0]  \n",
       "4         easy  [1, 0, 0]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_proc_df = merged_text_and_token_list_df(df)\n",
    "res_df = tokenize_and_process(to_proc_df)\n",
    "res_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample(df, sample_sizes):\n",
    "    classes = df.difficulty.unique()\n",
    "    classes_sample = []\n",
    "    \n",
    "    for cls in classes:\n",
    "        class_df = df[df['difficulty'] == cls]\n",
    "        if cls in sample_sizes:\n",
    "            sample_size = sample_sizes[cls]\n",
    "            sampled_df = class_df.sample(sample_size, replace=False) if sample_size < len(class_df) else class_df\n",
    "            classes_sample.append(sampled_df)\n",
    "    \n",
    "    final_df = pd.concat(classes_sample).reset_index(drop=True)\n",
    "    return final_df\n",
    "\n",
    "def oversample(df, sample_sizes):\n",
    "    classes = df.difficulty.unique()\n",
    "    classes_sample = []\n",
    "    \n",
    "    for cls in classes:\n",
    "        class_df = df[df['difficulty'] == cls]\n",
    "        if cls in sample_sizes:\n",
    "            sample_size = sample_sizes[cls]\n",
    "            sampled_df = class_df.sample(sample_size, replace=True) if sample_size > len(class_df) else class_df\n",
    "            classes_sample.append(sampled_df)\n",
    "    \n",
    "    final_df = pd.concat(classes_sample).reset_index(drop=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='color:red'>STRATIFIED CLASS DISTRIBUTION with</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPORTION OF TARGET IN THE ORIGINAL DATA\n",
      "difficulty\n",
      "easy           0.675756\n",
      "medium         0.271978\n",
      "challenging    0.052265\n",
      "Name: count, dtype: float64\n",
      "\n",
      "PROPORTION OF TARGET IN THE TRAINING SET\n",
      "difficulty\n",
      "easy           0.675821\n",
      "medium         0.271938\n",
      "challenging    0.052241\n",
      "Name: count, dtype: float64\n",
      "\n",
      "PROPORTION OF TARGET IN THE TEST SET\n",
      "difficulty\n",
      "easy           0.675177\n",
      "medium         0.272340\n",
      "challenging    0.052482\n",
      "Name: count, dtype: float64\n",
      "Class weights [0.49322746 1.22576901 6.38066465]\n",
      "Initial training dataset shape Counter({'easy': 4282, 'medium': 1723, 'challenging': 331})\n",
      "Undersampled dataset shape Counter({'easy': 3000, 'medium': 1250, 'challenging': 331})\n",
      "Final dataset shape Counter({'easy': 3000, 'medium': 1250, 'challenging': 600})\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:red'>OVER/UNDERSAMPLED CLASS DISTRIBUTION with</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPORTION OF TARGET IN THE TRAINING SET\n",
      "difficulty\n",
      "easy           0.618557\n",
      "medium         0.257732\n",
      "challenging    0.123711\n",
      "Name: count, dtype: float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printmd(\"STRATIFIED CLASS DISTRIBUTION with\")\n",
    "strat_train_df,strat_test_df = train_test_split(res_df,test_size=0.1,stratify=res_df['difficulty'],random_state=11)\n",
    "print(f'PROPORTION OF TARGET IN THE ORIGINAL DATA\\n{res_df[\"difficulty\"].value_counts() / len(res_df)}\\n\\n'+\n",
    "      f'PROPORTION OF TARGET IN THE TRAINING SET\\n{strat_train_df[\"difficulty\"].value_counts() / len(strat_train_df)}\\n\\n'+\n",
    "      f'PROPORTION OF TARGET IN THE TEST SET\\n{strat_test_df[\"difficulty\"].value_counts() / len(strat_test_df)}')\n",
    "\n",
    "from collections import Counter\n",
    "classes = np.array([\"easy\",\"medium\",\"challenging\"])  # Example classes\n",
    "c_labels = strat_train_df[\"difficulty\"] # Labels from the dataset\n",
    "CLASS_WEIGHTS = compute_class_weight(class_weight='balanced', classes=classes, y=c_labels)\n",
    "print(\"Class weights\", CLASS_WEIGHTS)\n",
    "\n",
    "# Define desired sample sizes for each class\n",
    "undersample_sizes = {'easy': 3000, 'medium': 1250, 'challenging': 331}\n",
    "oversample_sizes = {'easy': 3000, 'medium': 1250, 'challenging': 600}\n",
    "\n",
    "print('Initial training dataset shape %s' % Counter(strat_train_df[\"difficulty\"]))\n",
    "# Apply undersampling\n",
    "under_strat_train_df = undersample(strat_train_df, undersample_sizes)\n",
    "print('Undersampled dataset shape %s' % Counter(under_strat_train_df[\"difficulty\"]))\n",
    "# Apply oversampling\n",
    "fin_strat_train_df  = oversample(under_strat_train_df, oversample_sizes)\n",
    "print('Final dataset shape %s' % Counter(fin_strat_train_df[\"difficulty\"]))\n",
    "\n",
    "printmd(\"OVER/UNDERSAMPLED CLASS DISTRIBUTION with\")\n",
    "print(f'PROPORTION OF TARGET IN THE TRAINING SET\\n{fin_strat_train_df[\"difficulty\"].value_counts() / len(fin_strat_train_df)}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingDataset(Dataset):\n",
    "    def __init__(self, p_df, word_to_idx):\n",
    "        self.p_df = p_df\n",
    "        self.word_to_idx = word_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.p_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.p_df['token_list'].iloc[idx]\n",
    "        label = self.p_df['label'].iloc[idx]  # Assuming 'label' is the column name\n",
    "        numerical_seq = [self.word_to_idx[word] for word in sentence]\n",
    "        encoded_label = torch.tensor(label, dtype=torch.float32)  # Ensure float32 for BCE loss\n",
    "        return torch.tensor(numerical_seq), encoded_label\n",
    "    \n",
    "WORD_TO_INDEX = {word: idx for idx, word in enumerate(set(word for sentence in res_df[\"token_list\"] for word in sentence))}\n",
    "MAX_SEQ_LEN = max([len(sentence) for sentence in res_df[\"token_list\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifierT(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, fc_layers = 1, num_layers=1, bidirectional=False, dropout=0.2):\n",
    "        super(RNNClassifierT, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Embedding layer\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, \n",
    "                          dropout=dropout, batch_first=True, \n",
    "                          bidirectional=bidirectional)  # RNN layer\n",
    "        \n",
    "        # If bidirectional, the output size of the RNN will be doubled (since it has both forward and backward states)\n",
    "        if bidirectional:\n",
    "            rnn_output_dim = hidden_dim * 2\n",
    "        else:\n",
    "            rnn_output_dim = hidden_dim\n",
    "        \n",
    "        self.fc = nn.ModuleList()\n",
    "        if fc_layers > 1:\n",
    "            for i in range(fc_layers - 1):\n",
    "                self.fc.append(nn.Linear(rnn_output_dim, rnn_output_dim))  # Fully connected layer for classification\n",
    "        \n",
    "        self.final_fc = nn.Linear(rnn_output_dim, num_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)  # Dropout layer to prevent overfitting\n",
    "        print(\"model: \",self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through embedding layer\n",
    "        embedded = self.embedding(x)\n",
    "        if DEBUG:\n",
    "            print(\"embedded shape:\", embedded.shape)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # Pass through RNN\n",
    "        _, h_n = self.rnn(embedded)  # Only interested in hidden states (h_n)\n",
    "        if DEBUG:\n",
    "            print(\"RNN output shape:\", h_n.shape)  # Shape: (num_layers * num_directions, batch_size, hidden_dim)\n",
    "\n",
    "        # Handle multi-layer RNN and bidirectional case\n",
    "        if self.rnn.bidirectional:\n",
    "            # Separate forward and backward states from the last layer\n",
    "            forward_h = h_n[-2]  # Forward hidden state of the last layer\n",
    "            backward_h = h_n[-1]  # Backward hidden state of the last layer\n",
    "            h_n = torch.cat((forward_h, backward_h), dim=1)  # Concatenate along feature dimension\n",
    "        else:\n",
    "            # Use the last hidden state from the last layer (for unidirectional RNN)\n",
    "            h_n = h_n[-1]\n",
    "\n",
    "        if DEBUG:\n",
    "            print(\"After processing hidden state shape:\", h_n.shape)  # Shape: (batch_size, hidden_dim * num_directions)\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        for layer in self.fc:\n",
    "            h_n = F.relu(layer(h_n))\n",
    "            if DEBUG:\n",
    "                print(\"After FC shape:\", h_n.shape)\n",
    "            h_n = self.dropout(h_n)\n",
    "            if DEBUG:\n",
    "                print(\"After dropout shape:\", h_n.shape)\n",
    "\n",
    "        # Final output layer\n",
    "        output = self.final_fc(h_n)\n",
    "        if DEBUG:\n",
    "            print(\"After final FC (output) shape:\", output.shape)  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        # Apply softmax for multi-class classification\n",
    "        return torch.softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim=64, num_classes=3, fc_layers = 1, num_layers=1, bidirectional=False, dropout=0.2):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Embedding layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.final_fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)  # Dropout layer to prevent overfitting\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        print(\"this is the lstm input:\",sentence)\n",
    "        embeds = self.embedding(sentence)\n",
    "        print(\"this is the embeddings:\",embeds)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        print(\"this is the lstm output:\",lstm_out)\n",
    "        tag_space = self.final_fc(lstm_out.view(len(sentence), -1))\n",
    "        print(\"this is the final outputs:\",tag_space)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        print(\"output probs:\",tag_scores)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=4, delta=0, path='checkpoint.pt', verbose=False, monitor=[\"val_loss\"], combine_metrics=\"weighted_average\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after the last improvement in the monitored metric.\n",
    "            delta (float): Minimum change in the monitored metric to qualify as an improvement.\n",
    "            path (str): Path to save the best model.\n",
    "            verbose (bool): If True, prints updates when metrics improve.\n",
    "            monitor (list): List of metrics to monitor, e.g., [\"val_loss\", \"f1_macro\", \"f1_weighted\"].\n",
    "            combine_metrics (str): Method for combining metrics. Options: \"weighted_average\", \"average\".\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_metric = None\n",
    "        self.monitor = monitor\n",
    "        self.combine_metrics = combine_metrics\n",
    "\n",
    "    def __call__(self, metrics, model):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            metrics (dict): Dictionary of metrics to monitor, e.g., {'val_loss': 0.5, 'f1_macro': 0.75, 'f1_weighted': 0.65}.\n",
    "            model (nn.Module): The model being trained.\n",
    "        \"\"\"\n",
    "        # Extract the values for the monitored metrics\n",
    "        metric_values = {metric: metrics.get(metric) for metric in self.monitor}\n",
    "\n",
    "        # If any monitored metric is missing, raise an error\n",
    "        if None in metric_values.values():\n",
    "            raise ValueError(f\"One or more of the monitored metrics: {self.monitor} is missing in the provided metrics.\")\n",
    "\n",
    "        # Combine the metrics using the defined combination method\n",
    "        combined_score = self.combine_metrics_func(metric_values)\n",
    "\n",
    "        # For metrics where higher is better (like F1, precision, recall), we flip the sign to work with \"score\".\n",
    "        if \"val_loss\" in self.monitor:\n",
    "            score = -combined_score  # Minimize loss\n",
    "        else:\n",
    "            score = combined_score  # Maximize F1, precision, recall, etc.\n",
    "\n",
    "        # Initialize best_score on first call\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(metrics, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(metrics, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def combine_metrics_func(self, metric_values):\n",
    "        \"\"\"\n",
    "        Combines the given metrics into a single score.\n",
    "        - 'weighted_average': Calculates a weighted average of the metrics.\n",
    "        - 'average': Takes the average of the metrics.\n",
    "        \"\"\"\n",
    "        if self.combine_metrics == \"weighted_average\":\n",
    "            # Prioritize F1 macro, F1 weighted, and recall (for minority class focus)\n",
    "            weights = {\"f1_macro\": 0.4, \"f1_weighted\": 0.4, \"recall_macro\": 0.2}  # Adjust weights to emphasize minority class\n",
    "            weighted_sum = sum([metric_values.get(k, 0) * weights.get(k, 1) for k in metric_values])\n",
    "            total_weight = sum([weights.get(k, 1) for k in metric_values])\n",
    "            print(f\"Current Score:{weighted_sum / total_weight if total_weight > 0 else 0}\")\n",
    "            return weighted_sum / total_weight if total_weight > 0 else 0\n",
    "        \n",
    "        elif self.combine_metrics == \"average\":\n",
    "            return sum(metric_values.values()) / len(metric_values)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown combination method: {self.combine_metrics}\")\n",
    "\n",
    "    def save_checkpoint(self, metrics, model):\n",
    "        \"\"\"Saves the model if the monitored metric improves.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"Metrics improved. Saving model with metrics: {metrics}\")\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.best_metric = metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(word_to_idx, d_loader, v_loader, max_epoch = 1, rnn_layers = 1, rnn_d = 0, wandb_run = None, fc_layers=1, \n",
    "               embed_dim = 100, hidden_dim = 64, bidirect = False, lr = 0.001, op = \"Adam\",model_n=\"RNN\"):\n",
    "    \n",
    "    vocab_size = len(word_to_idx)\n",
    "    output_size = 3  # Assuming 3 classes\n",
    "\n",
    "    # Initialize EarlyStopping with metrics to monitor\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=2, \n",
    "        monitor=[\"val_loss\", \"f1_macro\", \"f1_weighted\", \"recall_macro\"], \n",
    "        combine_metrics=\"weighted_average\", \n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    t_model = None\n",
    "    # Define the model\n",
    "    if model_n == \"RNN\":\n",
    "        t_model = RNNClassifierT(vocab_size, \n",
    "                                 embedding_dim=embed_dim, \n",
    "                                 hidden_dim=hidden_dim, \n",
    "                                 num_classes = 3, \n",
    "                                 num_layers = rnn_layers, \n",
    "                                 fc_layers = fc_layers,\n",
    "                                 dropout = rnn_d,\n",
    "                                 bidirectional = bidirect)\n",
    "    if model_n == \"LSTM\":\n",
    "        t_model = LSTMClassifier(vocab_size, \n",
    "                                 embedding_dim=embed_dim, \n",
    "                                 hidden_dim=hidden_dim, \n",
    "                                 num_classes = 3, \n",
    "                                 num_layers = rnn_layers, \n",
    "                                 fc_layers = fc_layers,\n",
    "                                 dropout = rnn_d,\n",
    "                                 bidirectional = bidirect)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCELoss(torch.tensor(CLASS_WEIGHTS))\n",
    "    optimizer = Adam(t_model.parameters(), lr=lr) if op == \"Adam\" else RMSprop(t_model.parameters(), lr=lr)\n",
    "\n",
    "    epoch = 0\n",
    "    for i in range(max_epoch):\n",
    "        t_model.train()\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        t_losses = []\n",
    "        v_losses = []\n",
    "        train_loss = 0.0\n",
    "        for i, batch in enumerate(tqdm(d_loader)):\n",
    "            inputs, labels = batch\n",
    "            padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=0)  \n",
    "            optimizer.zero_grad()            \n",
    "            outputs = t_model(padded_inputs)#SQUEEEZZEEE WAS ADDED BY CHATGPT CARE\n",
    "            loss = criterion(outputs, labels.float())  # Reshape labels to match output size\n",
    "            train_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        average_train_loss = train_loss / len(d_loader)\n",
    "        t_losses.append(average_train_loss)\n",
    "        epoch += 1\n",
    "        t_model.eval()\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0        \n",
    "            for i, batch in enumerate(v_loader):  \n",
    "                inputs, labels = batch\n",
    "                v_outputs = t_model(inputs)\n",
    "                loss = criterion(v_outputs, labels.float())\n",
    "                val_loss += loss.item()\n",
    "                                        \n",
    "                all_predictions.extend(v_outputs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            average_val_loss = val_loss / len(v_loader)\n",
    "            v_losses.append(average_val_loss)\n",
    "            print(f\"Training loss: {average_train_loss}, Validation loss: {average_val_loss}\")\n",
    "             # Calculate metrics\n",
    "        \n",
    "            #RETURNS ONE HOT AND INT REPRESENTATION\n",
    "            all_predictions_hot, predicted_classes = predictions_to_one_hot(torch.Tensor(all_predictions))\n",
    "            all_labels_t = torch.Tensor(all_labels)\n",
    "            label_classes = torch.argmax(all_labels_t, dim=1) \n",
    "            \n",
    "            f1_macro = f1_score(all_labels_t, all_predictions_hot, average='macro')\n",
    "            f1_weighted = f1_score(all_labels_t, all_predictions_hot, average='weighted')\n",
    "            recall_macro = recall_score(all_labels_t, all_predictions_hot, average='macro')\n",
    "            accuracy = accuracy_score(all_labels_t, all_predictions_hot)\n",
    "\n",
    "            # Pack metrics into a dictionary\n",
    "            metrics = {\n",
    "                'val_loss': average_val_loss,\n",
    "                'f1_macro': f1_macro,\n",
    "                'f1_weighted': f1_weighted,\n",
    "                'recall_macro': recall_macro,\n",
    "                'accuracy': accuracy\n",
    "            }\n",
    "\n",
    "            early_stopping(metrics, t_model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break \n",
    "            # Log metrics to W&B\n",
    "            \n",
    "            if wandb_run:\n",
    "                wandb_run.log({\n",
    "                    \"train_loss\": average_train_loss,\n",
    "                    'val_loss': average_val_loss,\n",
    "                    'f1_macro': f1_macro,\n",
    "                    'f1_weighted': f1_weighted,\n",
    "                    'recall_macro': recall_macro,\n",
    "                    'accuracy': accuracy,\n",
    "                    \"epoch\": epoch,\n",
    "                })\n",
    "        \n",
    "        printmd(f\"Epoch {epoch+1}: Train Loss = {train_loss}, Validation Loss = {val_loss}\")\n",
    "    \n",
    "    # Load the best model weights\n",
    "    t_model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    return t_model, t_losses, v_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_to_one_hot(predictions):\n",
    "    # Get the indices of the highest probability for each prediction\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)  # Returns indices of max probability for each sample\n",
    "    # Convert these indices to one-hot encoded format\n",
    "    one_hot_predictions = torch.zeros(predictions.size(0), predictions.size(1))  # Initialize a zero tensor\n",
    "    one_hot_predictions.scatter_(1, predicted_classes.unsqueeze(1), 1)  # Set the correct class to 1\n",
    "    return one_hot_predictions, predicted_classes\n",
    "\n",
    "def eval_loop(e_d_loader, e_model, epoch=0):\n",
    "    e_model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        for i, batch in enumerate(e_d_loader):  \n",
    "            inputs, labels = batch\n",
    "            padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=0)  \n",
    "            outputs = e_model(padded_inputs)\n",
    "            debug = False\n",
    "            if debug:\n",
    "                print(\"num in batch\",len(inputs))\n",
    "                print(\"Inputs\", inputs)   \n",
    "                print(\"outputs\", outputs)\n",
    "                print(\"labels\", labels.float())\n",
    "                print(\"labels re shaped\", labels.float().view(-1, 3))\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        \n",
    "        #RETURNS ONE HOT AND INT REPRESENTATION\n",
    "        all_predictions_hot, predicted_classes = predictions_to_one_hot(torch.Tensor(all_predictions))\n",
    "        all_labels_t = torch.Tensor(all_labels)\n",
    "        label_classes = torch.argmax(all_labels_t, dim=1) \n",
    "        \n",
    "        avg_loss = total_loss / len(e_d_loader)\n",
    "\n",
    "        # Ensure that all_labels and all_predictions are numpy arrays and in correct format\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "     \n",
    "        # Calculate F1-score for multi-label classification\n",
    "        f1_score_macro = f1_score(all_labels, all_predictions_hot, average='macro')\n",
    "        f1_score_micro = f1_score(all_labels, all_predictions_hot, average='micro')\n",
    "        f1_score_weighted = f1_score(all_labels, all_predictions_hot, average='weighted')\n",
    "        f1_score_samples = f1_score(all_labels, all_predictions_hot, average='samples')\n",
    "         \n",
    "        # Calculate Accuracy\n",
    "        accuracy = accuracy_score(all_labels, all_predictions_hot)\n",
    "        \n",
    "        # Calculate Precision\n",
    "        precision_macro = precision_score(all_labels, all_predictions_hot, average='macro', zero_division=0)\n",
    "        precision_micro = precision_score(all_labels, all_predictions_hot, average='micro', zero_division=0)\n",
    "        precision_weighted = precision_score(all_labels, all_predictions_hot, average='weighted', zero_division=0)       \n",
    "        \n",
    "        #confusion matrix\n",
    "        class_names = [\"Easy\", \"Medium\", \"Challenging\"]\n",
    "        cm = confusion_matrix(label_classes, predicted_classes)\n",
    "        # Create a DataFrame for better visualization\n",
    "        cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "   \n",
    "        # Print metrics\n",
    "        printmd(f\"Epoch {epoch+1}\")\n",
    "        printmd(f\"Validation Loss: {avg_loss:.4f}\\n\")\n",
    "        printmd(f\"Accuracy: {accuracy:.4f}\")\n",
    "        printmd(f\"Precision (Macro): {precision_macro:.4f}, Precision (Micro): {precision_micro:.4f}\")\n",
    "        printmd(f\"Precision (Weighted): {precision_weighted:.4f}\\n\")\n",
    "        printmd(f\"F1-Score (Macro): {f1_score_macro:.4f}, F1-Score (Micro): {f1_score_micro:.4f}\")\n",
    "        printmd(f\"F1-Score (Weighted): {f1_score_weighted:.4f}, F1-Score (Samples): {f1_score_samples:.4f}\\n\")\n",
    "        printmd(f\"Multilabel Confusion Matrix:\\n{cm_df}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method in dataloader to pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to apply padding per batch.\n",
    "    \"\"\"\n",
    "    inputs, labels = zip(*batch)\n",
    "    \n",
    "    # Pad the input sequences to the same length\n",
    "    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=0)  # pad with 0\n",
    "    \n",
    "    # Stack the labels (assuming labels are already one-hot encoded or scalar labels)\n",
    "    padded_labels = torch.stack(labels, dim=0)\n",
    "    \n",
    "    return padded_inputs, padded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WORD_TO_IDX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset_h \u001b[38;5;241m=\u001b[39m WordEmbeddingDataset(fin_strat_train_df, WORD_TO_IDX)\n\u001b[0;32m      2\u001b[0m test_dataset_h \u001b[38;5;241m=\u001b[39m  WordEmbeddingDataset(strat_test_df, WORD_TO_IDX)  \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create the DataLoader\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WORD_TO_IDX' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset_h = WordEmbeddingDataset(fin_strat_train_df, WORD_TO_IDX)\n",
    "test_dataset_h =  WordEmbeddingDataset(strat_test_df, WORD_TO_IDX)  \n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader = DataLoader(train_dataset_h, batch_size=32, shuffle=True,collate_fn=pad_collate)\n",
    "val_dataloader = DataLoader(test_dataset_h, batch_size=32, shuffle=True, collate_fn=pad_collate)\n",
    "DEBUG = False  \n",
    "rm, _, _ = train_loop(WORD_TO_IDX, train_dataloader, val_dataloader, embed_dim =100,hidden_dim = 64,\n",
    "                      max_epoch = 10,rnn_layers = 3, rnn_d=0.2, fc_layers=3, bidirect=True, lr = 0.001, op=\"Adam\", model_n=\"RNN\")\n",
    "\n",
    "eval_loop(val_dataloader, rm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter space\n",
    "HYPER_PARAMS = {\n",
    "    #\"embedding_dim\": [50, 100, 200],\n",
    "    \"hidden_dim\": [32, 64, 128],\n",
    "    \"num_layers\": [1, 2, 3],\n",
    "    \"bidirectional\": [True, False],\n",
    "    \"dropout\": [0.3],\n",
    "    \"learning_rate\": [1e-3, 1e-4],\n",
    "    \"batch_size\": [16, 32, 64],\n",
    "    \"fc_layers\": [1,2,3],\n",
    "    \"optimizer\": [\"Adam\", \"RMSprop\"]\n",
    "}\n",
    "\n",
    "train_dataset = WordEmbeddingDataset(fin_strat_train_df, WORD_TO_IDX)\n",
    "test_dataset =  WordEmbeddingDataset(strat_test_df, WORD_TO_IDX)  \n",
    "# Create the DataLoader\n",
    "\n",
    "# Iterate through combinations\n",
    "def iterate_combinations(params_dict):\n",
    "    keys = list(params_dict.keys())\n",
    "    values = list(params_dict.values())\n",
    "    total_combinations = 1\n",
    "    for v in values:\n",
    "        total_combinations *= len(v)\n",
    "\n",
    "    print(f\"Total combinations: {total_combinations}\")\n",
    "\n",
    "    # Initialize visualization tools\n",
    "    wandb.init(project=\"hyperparameter_tuning\", name=\"all_combinations\")\n",
    "    \n",
    "    # Store results for final visualization\n",
    "    results = []\n",
    "\n",
    "    # Generate parameter combinations without itertools\n",
    "    def generate_combinations(idx=0, current_combination={}):\n",
    "        if idx == len(keys):\n",
    "            yield deepcopy(current_combination)\n",
    "            return\n",
    "        \n",
    "        key = keys[idx]\n",
    "        for value in params_dict[key]:\n",
    "            current_combination[key] = value\n",
    "            yield from generate_combinations(idx + 1, current_combination)\n",
    "\n",
    "    for combination in generate_combinations():\n",
    "        # Extract hyperparameters\n",
    "        #embedding_dim = combination[\"embedding_dim\"]\n",
    "        hidden_dim = combination[\"hidden_dim\"]\n",
    "        num_layers = combination[\"num_layers\"]\n",
    "        bidirectional = combination[\"bidirectional\"]\n",
    "        dropout = combination[\"dropout\"]\n",
    "        learning_rate = combination[\"learning_rate\"]\n",
    "        batch_size = combination[\"batch_size\"]\n",
    "        optimizer_name = combination[\"optimizer\"]\n",
    "        fc_layers = combination[\"fc_layers\"]\n",
    "        \n",
    "        print(f\"Current Combination: {combination}\")\n",
    "        \n",
    "        # Log hyperparameters to W&B\n",
    "        wandb.config.update(combination)\n",
    "        \n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,collate_fn=pad_collate)\n",
    "        val_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "        \n",
    "        rm,train_losses, val_losses = train_loop(WORD_TO_IDX, \n",
    "                        train_dataloader, \n",
    "                        val_dataloader, \n",
    "                        max_epoch = 10,\n",
    "                        rnn_layers = num_layers,\n",
    "                        rnn_d = dropout,\n",
    "                        wandb_run = wandb,\n",
    "                        fc_layers = fc_layers,\n",
    "                        embed_dim = 100,\n",
    "                        hidden_dim = hidden_dim,\n",
    "                        bidirect = bidirectional,\n",
    "                        lr = learning_rate,\n",
    "                        op = optimizer_name)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"final_train_loss\": train_losses[-1],\n",
    "            \"final_val_loss\": val_losses[-1],\n",
    "        })\n",
    "\n",
    "def visualize_results(results):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for res in results:\n",
    "        combination = res[\"combination\"]\n",
    "        train_loss = res[\"train_loss\"]\n",
    "        val_loss = res[\"val_loss\"]\n",
    "        label = f\"{combination}\"\n",
    "        plt.plot(train_loss, label=f\"Train - {label}\")\n",
    "        plt.plot(val_loss, label=f\"Val - {label}\")\n",
    "    \n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Hyperparameter Tuning Loss Trends\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "# Run parameter iteration\n",
    "iterate_combinations(HYPER_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (OLD) Prediction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(pmodel, text):\n",
    "    # Tokenize the input text\n",
    "    tokens = t_and_p_new_input(text)\n",
    "    \n",
    "    # Convert tokens to numerical sequence\n",
    "    numerical_seq = [WORD_TO_IDX.get(word, 0) for word in tokens]  # 0 for unknown words\n",
    "    max_seq_len = len(WORD_TO_IDX)\n",
    "    # Pad the sequence to the maximum length\n",
    "    # Ensure sequence length is at least max_seq_len\n",
    "    #padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=0)  \n",
    "    padded_seq = numerical_seq[:max_seq_len]\n",
    "    while len(padded_seq) < max_seq_len:\n",
    "        padded_seq.append(0)  # Pad with 0s\n",
    "    padded_seq = torch.tensor(padded_seq)\n",
    "    #padded_seq = pad_sequence([torch.tensor(numerical_seq)], padding_value=0, batch_first=True, max_length=max_seq_len)\n",
    "\n",
    "    # Create a batch and convert to a tensor\n",
    "    batch = [padded_seq]\n",
    "    batch = torch.stack(batch)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    pmodel.eval()\n",
    "    with torch.no_grad():\n",
    "        output =pmodel(batch)\n",
    "        print(\"model output\", output)\n",
    "        predicted_class = torch.argmax(output, dim=1)\n",
    "        print(\"predicted_class\", predicted_class)\n",
    "        return predicted_class.item()\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "##########################################\n",
    "######## UPDATE FOR NEW PADDING ####################################################\n",
    "################################################## UPDATE FOR NEW PADDING ##########\n",
    "                                          ##########################################  \n",
    "med_input = \"Title: Veal sweetbreads with ras el hanout, cauliflower purée, watercress and coriander\\nDescription: Marcus Eaves' beautiful sweetbread dish masterfully balances delicately flavoured sweetbreads and cauliflower with bright and bold coriander, pomegranate and mint.\\nCategory: Starter\\nTotal Time: 90\\nIngredients: 100g of veal sweetbreads, 70g of T45 flour, 5g of ras el hanout, 1 garlic clove, 1 sprig of thyme, 1 tbsp of butter, plus beurre noisette butter, 1 tsp pine nuts, parsley, chopped, lemon juice, vegetable oil, salt, pepper, 200g of cauliflower, finely chopped, 150g of milk, 100g of double cream, 3g of salt, 120ml of lemon olive oil, 30g of lemon vinegar, salt to taste, 150g of cauliflower, grated on a microplane, salt, 2 tbsp of golden raisins, soaked, 1 tbsp of pine nuts, 1 tbsp of pomegranate seeds, mint leaves, chopped, coriander, finely chopped, 1 cauliflower, butter, 30ml of chicken stock, baby watercress, washed and picked, coriander, olive oilInstruction: Start by making the cauliflower crisp to garnish the dish. Use a mandoline to cut 2mm thick slices of half of the cauliflower set aside for garnish. Lay the slices out onto a non-stick mat and place in a dehydrator at 60°C until the cauliflower is crisp, for approximately 4-6 hours. For the cauliflower purée, add the milk, double cream and salt to a pan and bring to a simmer. Add the cauliflower and cook until tender and soft to the touch, this will take 5-10 minutes. Allow to cook until about half of the liquid has evaporated, then tip the contents of the pan into a strainer, saving the liquid. Blend the cauliflower with half of the remaining liquid, adding more to achieve the desired consistency. Pass through a fine sieve and set aside until ready to serve. For the lemon vinaigrette, simply whisk together the ingredients until smooth. Ensure to mix well again before using later. For the cauliflower couscous, cook the cauliflower in salted boiling water until tender for 2-3 minutes, strain and refresh in ice water. Drain and mix in the golden raisins and pine nuts. Toss to combine and season to taste with the lemon vinaigrette. Set aside. To cook the sweetbreads, preheat a water bath to 63 ̊C. Season evenly with salt and pepper and vacuum seal. Cook in the water bath for 11 minutes and cool in ice cold water. Then, remove from the pouch and pat dry. Season the cooked sweetbreads and roll in a mix of the flour and ras el hanout. Add a generous film of olive oil to a pan and place over a high heat. Once hot, caramelise the sweetbreads on one side and turn over. Add a tablespoon of the butter, the garlic and the thyme sprigs and use a spoon to continuously baste the sweetbreads in the foaming butter for another 1-2 minutes. Remove the sweetbreads from the pan, rest for 45 seconds then carve. Drain the fat from the pan, wipe dry with kitchen paper and return to the heat. Make a beurre noisette with another knob of butter and add the pine nuts, chopped parsley and a generous squeeze of lemon juice. Remove from the heat and set aside. Cut the remaining cauliflower half for garnish into medium sized florets. Blanch the pieces of cauliflower in salted boiling water for up to 1 minute, then remove and set aside to dry. Place a small pan over a medium heat and add a knob of butter and the chicken stock. Reduce to a glaze, stirring with a wooden spoon to maintain its texture. Remove from heat and place in a small bowl. Roll the blanched cauliflower through this chicken stock and butter emulsion until well-coated. To serve, add the fresh pomegranate, mint and coriander to the cauliflower couscous. Place one heaped tablespoon of the couscous on each plate and place the buttered cauliflower around the edge. Add slices of sweetbread to cover the cauliflower couscous. Dress the sweetbreads with the beurre noisette and decorate the plate with baby watercress, picked coriander leaves and dehydrated cauliflower. Finish with quenelles of the cauliflower purée and a drizzle of olive oil\"\n",
    "chal_input = \"Title: Cured sea trout with garden pea, nasturtium and langoustine dashi\\nDescription: Phil Fanning celebrates spring's finest fish with great style in this recipe, featuring cured fillets and a sea trout tortellini \\nCategory: Main\\nTotal Time: 150\\nIngredients: 250g of 00 flour, 150g of egg yolk, 15g of milk, 13g of olive oil, 1g of ground sea salt, 2 bunches of fresh tarragon, 200g of caster sugar, 200g of grey salt, 50g of dried shiitake mushrooms, 500g of sea trout, skinless, 88g of sea trout, frozen, 7g of Maldon salt, 15g of egg white, 150g of whipping cream, 100g of frozen peas, 500g of langoustine shells, washed and crushed, 500g of fish stock, 100g of white chicken stock, 50g of leek, washed and sliced, 150g of banana shallot, sliced, 70g of sake, 1 kombu, sheet, 25g of shiitake mushrooms, 1 dash of soy sauce, to taste, 1 bunch of fresh tarragon, to refresh, rice vinegar, dash, egg white, to clarify, 30g of garlic, peeled, 500 morels, 600g of white wine, 340g of white balsamic vinegar, 900ml of ice cold water, 40g of kombu, 150g of caster sugar, 10g of black peppercorns, 24g of fresh thyme, 8g of fresh tarragon, 8 bay leaves, 1 handful of nasturtium flowers and leaves, 1 handful of fresh peas, trout roe, to garnishInstruction: To make the pasta, bring all of the ingredients together and knead well for 10 minutes. Place the dough in a bag and rest in the fridge for 30 minutes. Place the tarragon, sugar, salt and shiitakes in a food processor and blitz for 1-2 minutes. Rub the marinade into the fish and leave to cure for 1 hour. After 1 hour, wash the marinade off and pat dry\\r\\n. Portion the fish and reserve until ready to cook. To make the trout filling, blitz the frozen trout in a food processor to a fine powder. Scrape down the edges of the bowl, then add the egg white and continue to blitz until you have a smooth trout purée. Transfer this purée to a bowl over ice and fold the cream in without over-working. Add the salt and mix well. Place in the fridge for 10 minutes, then fold in the peas and chill. To pickle the morels, bring all of the wet ingredients to a boil in a medium sized pan. Add the sugar and spices and stir to combine. Add the garlic, remove from the heat and leave to cool. Add the fresh herbs and mushrooms, seal in a vac pac bag and leave to sit for a couple of hours, or until ready to plate. To make the langoustine dashi, add the fish stock to a medium sized pan. Reduce by half over a medium heat and set aside until ready to use. Drain the langoustine shells well and pan-fry in a medium pan in a little oil until they have dried and are starting to colour. Add the shallots and leeks to the crushed shells and cook for 5 minutes until the onions have softened. Add the sake and cook for a further minute until all of the alcohol has been burnt off. Add the stocks to a large pan over a medium heat and, 1/3 at a time, add the shells, stirring to combine. Add the kombu, shiitake, rice vinegar and soy, bring to a simmer and remove from the heat. Clairfy with egg white, pass through a muslin cloth and refresh with tarragon leaves. Set aside until ready to use. Remove the pasta from the fridge and roll flat with a rolling pin. Using a pasta machine, continue to progressively stretch and thin your pasta dough until it is as thin as a piece of paper. Using a 5 inch diameter metal cutter, cut circles out of the sheet of pasta. Remove the trout filling from the fridge and while still cold, add a tablespoon of the mousse to each round of pasta, being careful not to overfill the tortellini. Lightly dip your finger into water and run it along the edge of the round pasta shape to moisten the edges. Fold the dough over to form an inflated 'D' and then pull the edges together to form a crossed bow. Dust with flour and set aside on a baking tray dusted liberally with flour. Preheat the oven to 160°C/gas mark 3. Lightly dress the cured trout fillets in oil and place in the oven for 15-20 minutes, until lightly baked and flaking when touched. Bring a pan of salted water to the boil, reduce to a delicate simmer and carefully add the tortellini. Simmer for 2-3 minutes or until al dente. Carefully remove from the water and drain. When ready to plate, reheat the dashi at a simmer. Place the trout in the middle of the plate, add the tortellini and a pickled morel. Add a dash of warm dashi and finish with fresh peas, roe and nasturtium flowers\"\n",
    "predicted_class = predict(rm,chal_input)\n",
    "print(\"Predicted Class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchviz import make_dot\n",
    "\"\"\"\n",
    "# create some sample input data\n",
    "example_x,example_y = train_dataset[0]\n",
    "print(example_x)\n",
    "print(example_y)\n",
    "\n",
    "output = res_model(example_x)\n",
    "\n",
    "# generate a model architecture visualization\n",
    "make_dot(output.mean(),\n",
    "         params=dict(res_model().named_parameters()),\n",
    "         show_attrs=True,\n",
    "         show_saved=True).render(\"MyPyTorchModel_torchviz\", format=\"png\")\n",
    "\"\"\"\n",
    "# create some sample input data\n",
    "#example_x, example_y = train_dataset[0]\n",
    "# Create a dummy input tensor\n",
    "batch_size = 32\n",
    "VOCAB_SIZE = len(WORD_TO_IDX)\n",
    "dummy_input = torch.randint(0, VOCAB_SIZE, (batch_size, 100))\n",
    "\n",
    "# Forward pass\n",
    "out = rm(dummy_input)\n",
    "\n",
    "#ENSURING VALS ARE CORRECT SHAPE\n",
    "print(dummy_input.shape)  # Should be (batch_size, MAX_SEQ_LEN)\n",
    "print(rm)          # Check model structure to confirm expected input shape\n",
    "\n",
    "#ENSURING DEVICES ARE THE SAME\n",
    "dummy_input = dummy_input.to('cpu')  # or 'cuda' if using GPU\n",
    "res_model = rm.to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the computation graph\n",
    "graph = make_dot(out, params=dict(res_model.named_parameters()))\n",
    "graph.render(\"RNNClassifier_Architecture\", format=\"png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
